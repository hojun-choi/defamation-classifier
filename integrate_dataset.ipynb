{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a26ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 100개, train: 1436개\n",
      "merged(중복제거 후): 1536개\n",
      "saved -> merged_dataset.jsonl\n",
      "sample keys: ['problem_situation', 'participants', 'raw_id', 'case_names', 'case_type', 'court_level', 'defendant', 'label', 'sentence_type', 'sentence_value', 'sentence_suspension', 'sentence_additional_order', 'sentence_reason', 'sentence_judgment']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "test_path  = Path(\"test_dataset.jsonl\")\n",
    "train_path = Path(\"train_dataset.jsonl\")\n",
    "out_path   = Path(\"merged_dataset.jsonl\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) 한글 -> 영어 키 매핑\n",
    "# -----------------------------\n",
    "TOP_KEY_MAP = {\n",
    "    \"문제상황\": \"problem_situation\",\n",
    "    \"등장인물\": \"participants\",\n",
    "    \"형량\": \"sentence\",          # flatten 하면서 제거됨\n",
    "    \"사건명\": \"case_name\",\n",
    "    \"사건유형\": \"case_type\",\n",
    "    \"사건종류\": \"case_type\",\n",
    "    \"법원단계\": \"court_level\",\n",
    "    \"레벨\": \"court_level\",\n",
    "    \"피고인\": \"defendant\",\n",
    "    \"피해자\": \"victim\",\n",
    "    \"판례명\": \"precedent_title\",\n",
    "    \"판결요지\": \"decision_summary\",\n",
    "    \"판결이유\": \"decision_reason\",\n",
    "    \"죄명\": \"charges\",\n",
    "    \"라벨\": \"label\",\n",
    "    \"정답\": \"label\",\n",
    "    \"b\": \"label\",               # 너 데이터 라벨 필드로 보여서 label로 통일\n",
    "    \"casenames\": \"case_names\",\n",
    "    \"casetype\": \"case_type\",\n",
    "    \"level\": \"court_level\",\n",
    "    # \"id\"는 별도로 raw_id로 처리할 거라 여기엔 안 넣어도 됨\n",
    "}\n",
    "\n",
    "SENTENCE_KEY_MAP = {\n",
    "    \"종류\": \"sentence_type\",\n",
    "    \"기간\": \"sentence_value\",          \n",
    "    \"집행유예\": \"sentence_suspension\",\n",
    "    \"부가명령\": \"sentence_additional_order\",\n",
    "    \"이유\": \"sentence_reason\",\n",
    "    \"판단\": \"sentence_judgment\",\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 2) json / jsonl 로더\n",
    "# -----------------------------\n",
    "def load_json_any(path: Path):\n",
    "    text = path.read_text(encoding=\"utf-8-sig\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    if path.suffix.lower() == \".jsonl\":\n",
    "        items = []\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                items.append(json.loads(line))\n",
    "        return items\n",
    "\n",
    "    data = json.loads(text)\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict):\n",
    "        for k in [\"data\", \"items\", \"records\", \"dataset\"]:\n",
    "            if k in data and isinstance(data[k], list):\n",
    "                return data[k]\n",
    "        return [data]\n",
    "    return []\n",
    "\n",
    "# -----------------------------\n",
    "# 3) 키 변환 + 형량 flatten + id->raw_id\n",
    "# -----------------------------\n",
    "def normalize_item(x: dict):\n",
    "    out = {}\n",
    "\n",
    "    for k, v in x.items():\n",
    "        if k == \"id\":\n",
    "            out[\"raw_id\"] = v\n",
    "            continue\n",
    "        if k == \"raw_id\":  # 혹시 이미 raw_id면 그대로\n",
    "            out[\"raw_id\"] = v\n",
    "            continue\n",
    "\n",
    "        eng_k = TOP_KEY_MAP.get(k, k)  # 매핑 없으면 그대로 유지\n",
    "        out[eng_k] = v\n",
    "\n",
    "    # sentence(=형량) flatten\n",
    "    sent = None\n",
    "    if \"sentence\" in out and isinstance(out[\"sentence\"], dict):\n",
    "        sent = out.pop(\"sentence\")\n",
    "    elif \"형량\" in x and isinstance(x[\"형량\"], dict):\n",
    "        sent = x[\"형량\"]\n",
    "\n",
    "    if sent:\n",
    "        for sk, sv in sent.items():\n",
    "            eng_sk = SENTENCE_KEY_MAP.get(sk, f\"sentence_{sk}\")\n",
    "            out[eng_sk] = sv\n",
    "\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# 4) 로드 -> merge -> dedup(raw_id 기준) -> normalize\n",
    "# -----------------------------\n",
    "test_items  = load_json_any(test_path)\n",
    "train_items = load_json_any(train_path)\n",
    "\n",
    "merged_raw = test_items + train_items\n",
    "\n",
    "# raw_id(id) 기준 중복 제거\n",
    "seen = set()\n",
    "deduped_raw = []\n",
    "for obj in merged_raw:\n",
    "    rid = obj.get(\"raw_id\", obj.get(\"id\"))\n",
    "    if rid is None or rid not in seen:\n",
    "        deduped_raw.append(obj)\n",
    "        if rid is not None:\n",
    "            seen.add(rid)\n",
    "\n",
    "normalized = [normalize_item(obj) for obj in deduped_raw]\n",
    "\n",
    "# -----------------------------\n",
    "# 5) jsonl 저장\n",
    "# -----------------------------\n",
    "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for obj in normalized:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"test: {len(test_items)}개, train: {len(train_items)}개\")\n",
    "print(f\"merged(중복제거 후): {len(deduped_raw)}개\")\n",
    "print(f\"saved -> {out_path}\")\n",
    "print(\"sample keys:\", list(normalized[0].keys())[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0db6382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "  Using cached pymysql-1.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Using cached pymysql-1.1.2-py3-none-any.whl (45 kB)\n",
      "Installing collected packages: pymysql\n",
      "Successfully installed pymysql-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9990a6-4fce-4598-bdc3-86dfbc3b9d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000 rows...\n",
      "Inserted 1536 rows total.\n",
      "DONE ✅\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pymysql\n",
    "from pathlib import Path\n",
    "\n",
    "jsonl_path = Path(\"merged_dataset.jsonl\")  # ipynb 위치 기준\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"root\",\n",
    "    database=\"defamation\",\n",
    "    charset=\"utf8mb4\",\n",
    "    autocommit=False\n",
    ")\n",
    "\n",
    "insert_sql = \"\"\"\n",
    "INSERT INTO cases\n",
    "(raw_id, problem_situation, participants, case_names, case_type, court_level, defendant, label,\n",
    " sentence_type, sentence_value, sentence_suspension, sentence_additional_order, sentence_reason, sentence_judgment)\n",
    "VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "ON DUPLICATE KEY UPDATE\n",
    " problem_situation=VALUES(problem_situation),\n",
    " participants=VALUES(participants),\n",
    " case_names=VALUES(case_names),\n",
    " case_type=VALUES(case_type),\n",
    " court_level=VALUES(court_level),\n",
    " defendant=VALUES(defendant),\n",
    " label=VALUES(label),\n",
    " sentence_type=VALUES(sentence_type),\n",
    " sentence_value=VALUES(sentence_value),\n",
    " sentence_suspension=VALUES(sentence_suspension),\n",
    " sentence_additional_order=VALUES(sentence_additional_order),\n",
    " sentence_reason=VALUES(sentence_reason),\n",
    " sentence_judgment=VALUES(sentence_judgment)\n",
    "\"\"\"\n",
    "\n",
    "batch = []\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    with jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            participants_str = json.dumps(obj.get(\"participants\", []), ensure_ascii=False)\n",
    "            case_names_str   = json.dumps(obj.get(\"case_names\", []), ensure_ascii=False)\n",
    "\n",
    "            row = (\n",
    "                obj.get(\"raw_id\"),\n",
    "                obj.get(\"problem_situation\"),\n",
    "                participants_str,\n",
    "                case_names_str,\n",
    "                obj.get(\"case_type\"),\n",
    "                int(obj.get(\"court_level\")) if obj.get(\"court_level\") is not None else None,\n",
    "                obj.get(\"defendant\"),\n",
    "                int(obj.get(\"label\")) if obj.get(\"label\") is not None else None,\n",
    "                obj.get(\"sentence_type\"),\n",
    "                obj.get(\"sentence_value\"),\n",
    "                obj.get(\"sentence_suspension\"),\n",
    "                obj.get(\"sentence_additional_order\"),\n",
    "                obj.get(\"sentence_reason\"),\n",
    "                obj.get(\"sentence_judgment\"),\n",
    "            )\n",
    "            batch.append(row)\n",
    "\n",
    "            if len(batch) >= BATCH_SIZE:\n",
    "                cur.executemany(insert_sql, batch)\n",
    "                conn.commit()\n",
    "                print(f\"Inserted {i} rows...\")\n",
    "                batch.clear()\n",
    "\n",
    "        if batch:\n",
    "            cur.executemany(insert_sql, batch)\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {i} rows total.\")\n",
    "\n",
    "conn.close()\n",
    "print(\"DONE ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c95049-c76d-471a-b2ae-cb4408708902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
