{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a26ba4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_dataset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 104\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# 4) 로드 -> merge -> dedup(raw_id 기준) -> normalize\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m test_items  \u001b[38;5;241m=\u001b[39m load_json_any(test_path)\n\u001b[0;32m    105\u001b[0m train_items \u001b[38;5;241m=\u001b[39m load_json_any(train_path)\n\u001b[0;32m    107\u001b[0m merged_raw \u001b[38;5;241m=\u001b[39m test_items \u001b[38;5;241m+\u001b[39m train_items\n",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m, in \u001b[0;36mload_json_any\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json_any\u001b[39m(path: Path):\n\u001b[1;32m---> 48\u001b[0m     text \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mread_text(encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text:\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\pathlib.py:1027\u001b[0m, in \u001b[0;36mPath.read_text\u001b[1;34m(self, encoding, errors)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;124;03mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1027\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\pathlib.py:1013\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1012\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m, mode, buffering, encoding, errors, newline)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_dataset.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "test_path  = Path(\"test_dataset.jsonl\")\n",
    "train_path = Path(\"train_dataset.jsonl\")\n",
    "out_path   = Path(\"merged_dataset.jsonl\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) 한글 -> 영어 키 매핑\n",
    "# -----------------------------\n",
    "TOP_KEY_MAP = {\n",
    "    \"문제상황\": \"problem_situation\",\n",
    "    \"등장인물\": \"participants\",\n",
    "    \"형량\": \"sentence\",          # flatten 하면서 제거됨\n",
    "    \"사건명\": \"case_name\",\n",
    "    \"사건유형\": \"case_type\",\n",
    "    \"사건종류\": \"case_type\",\n",
    "    \"법원단계\": \"court_level\",\n",
    "    \"레벨\": \"court_level\",\n",
    "    \"피고인\": \"defendant\",\n",
    "    \"피해자\": \"victim\",\n",
    "    \"판례명\": \"precedent_title\",\n",
    "    \"판결요지\": \"decision_summary\",\n",
    "    \"판결이유\": \"decision_reason\",\n",
    "    \"죄명\": \"charges\",\n",
    "    \"라벨\": \"label\",\n",
    "    \"정답\": \"label\",\n",
    "    \"b\": \"label\",               # 너 데이터 라벨 필드로 보여서 label로 통일\n",
    "    \"casenames\": \"case_names\",\n",
    "    \"casetype\": \"case_type\",\n",
    "    \"level\": \"court_level\",\n",
    "    # \"id\"는 별도로 raw_id로 처리할 거라 여기엔 안 넣어도 됨\n",
    "}\n",
    "\n",
    "SENTENCE_KEY_MAP = {\n",
    "    \"종류\": \"sentence_type\",\n",
    "    \"기간\": \"sentence_value\",          \n",
    "    \"집행유예\": \"sentence_suspension\",\n",
    "    \"부가명령\": \"sentence_additional_order\",\n",
    "    \"이유\": \"sentence_reason\",\n",
    "    \"판단\": \"sentence_judgment\",\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 2) json / jsonl 로더\n",
    "# -----------------------------\n",
    "def load_json_any(path: Path):\n",
    "    text = path.read_text(encoding=\"utf-8-sig\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    if path.suffix.lower() == \".jsonl\":\n",
    "        items = []\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                items.append(json.loads(line))\n",
    "        return items\n",
    "\n",
    "    data = json.loads(text)\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict):\n",
    "        for k in [\"data\", \"items\", \"records\", \"dataset\"]:\n",
    "            if k in data and isinstance(data[k], list):\n",
    "                return data[k]\n",
    "        return [data]\n",
    "    return []\n",
    "\n",
    "# -----------------------------\n",
    "# 3) 키 변환 + 형량 flatten + id->raw_id\n",
    "# -----------------------------\n",
    "def normalize_item(x: dict):\n",
    "    out = {}\n",
    "\n",
    "    for k, v in x.items():\n",
    "        if k == \"id\":\n",
    "            out[\"raw_id\"] = v\n",
    "            continue\n",
    "        if k == \"raw_id\":  # 혹시 이미 raw_id면 그대로\n",
    "            out[\"raw_id\"] = v\n",
    "            continue\n",
    "\n",
    "        eng_k = TOP_KEY_MAP.get(k, k)  # 매핑 없으면 그대로 유지\n",
    "        out[eng_k] = v\n",
    "\n",
    "    # sentence(=형량) flatten\n",
    "    sent = None\n",
    "    if \"sentence\" in out and isinstance(out[\"sentence\"], dict):\n",
    "        sent = out.pop(\"sentence\")\n",
    "    elif \"형량\" in x and isinstance(x[\"형량\"], dict):\n",
    "        sent = x[\"형량\"]\n",
    "\n",
    "    if sent:\n",
    "        for sk, sv in sent.items():\n",
    "            eng_sk = SENTENCE_KEY_MAP.get(sk, f\"sentence_{sk}\")\n",
    "            out[eng_sk] = sv\n",
    "\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# 4) 로드 -> merge -> dedup(raw_id 기준) -> normalize\n",
    "# -----------------------------\n",
    "test_items  = load_json_any(test_path)\n",
    "train_items = load_json_any(train_path)\n",
    "\n",
    "merged_raw = test_items + train_items\n",
    "\n",
    "# raw_id(id) 기준 중복 제거\n",
    "seen = set()\n",
    "deduped_raw = []\n",
    "for obj in merged_raw:\n",
    "    rid = obj.get(\"raw_id\", obj.get(\"id\"))\n",
    "    if rid is None or rid not in seen:\n",
    "        deduped_raw.append(obj)\n",
    "        if rid is not None:\n",
    "            seen.add(rid)\n",
    "\n",
    "normalized = [normalize_item(obj) for obj in deduped_raw]\n",
    "\n",
    "# -----------------------------\n",
    "# 5) jsonl 저장\n",
    "# -----------------------------\n",
    "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for obj in normalized:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"test: {len(test_items)}개, train: {len(train_items)}개\")\n",
    "print(f\"merged(중복제거 후): {len(deduped_raw)}개\")\n",
    "print(f\"saved -> {out_path}\")\n",
    "print(\"sample keys:\", list(normalized[0].keys())[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db6382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
